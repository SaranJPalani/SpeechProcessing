{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ea7b6c",
   "metadata": {},
   "source": [
    "# Lab Assignment 4: Frame-wise Analysis of Speech Signal\n",
    "\n",
    "# Name: Saran Jayakumar Palani\n",
    "# Reg No: BL.EN.U4AIE23131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798074d",
   "metadata": {},
   "source": [
    "Task 1: Short-time processing of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"test.wav\"\n",
    "waveform, sr = librosa.load(audio_path, sr=16000, duration=20)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "librosa.display.waveshow(waveform, sr=sr)\n",
    "plt.title(\"Waveform of First 20 Seconds\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampling Rate: {sr} Hz\")\n",
    "print(f\"Duration: {len(waveform)/sr:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length_ms = 25\n",
    "frame_shift_ms = 10\n",
    "\n",
    "frame_length = int(frame_length_ms * sr / 1000)\n",
    "frame_shift = int(frame_shift_ms * sr / 1000)\n",
    "\n",
    "num_frames = int((len(waveform) - frame_length) / frame_shift) + 1\n",
    "\n",
    "print(f\"Frame Length: {frame_length} samples\")\n",
    "print(f\"Frame Shift: {frame_shift} samples\")\n",
    "print(f\"Total Frames: {num_frames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc343b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.zeros((num_frames, frame_length))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    start = i * frame_shift\n",
    "    end = start + frame_length\n",
    "    frames[i] = waveform[start:end]\n",
    "\n",
    "print(f\"Frame Matrix Shape: {frames.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(frames, window):\n",
    "    windowed = frames * window\n",
    "    \n",
    "    STE = np.sum(windowed**2, axis=1)\n",
    "    STM = np.sum(np.abs(windowed), axis=1)\n",
    "    \n",
    "    ZCR = np.zeros(len(windowed))\n",
    "    autocorr = []\n",
    "    AMDF = []\n",
    "    AMSDF = []\n",
    "    \n",
    "    for i, frame in enumerate(windowed):\n",
    "        ZCR[i] = np.sum(np.abs(np.diff(np.sign(frame)))) / (2 * len(frame))\n",
    "        \n",
    "        ac = np.correlate(frame, frame, mode='full')\n",
    "        ac = ac[len(ac)//2:]\n",
    "        autocorr.append(ac)\n",
    "        \n",
    "        amdf_vals = []\n",
    "        amsdf_vals = []\n",
    "        for k in range(len(frame)):\n",
    "            diff = frame[k:] - frame[:len(frame)-k]\n",
    "            amdf_vals.append(np.mean(np.abs(diff)))\n",
    "            amsdf_vals.append(np.mean(diff**2))\n",
    "        \n",
    "        AMDF.append(amdf_vals)\n",
    "        AMSDF.append(amsdf_vals)\n",
    "    \n",
    "    return STE, STM, ZCR, np.array(autocorr), np.array(AMDF), np.array(AMSDF)\n",
    "\n",
    "hamming_window = np.hamming(frame_length)\n",
    "STE_ham, STM_ham, ZCR_ham, AC_ham, AMDF_ham, AMSDF_ham = extract_features(frames, hamming_window)\n",
    "\n",
    "print(\"Hamming Window Features:\")\n",
    "print(f\"STE: {STE_ham.shape}, STM: {STM_ham.shape}, ZCR: {ZCR_ham.shape}\")\n",
    "print(f\"Autocorr: {AC_ham.shape}, AMDF: {AMDF_ham.shape}, AMSDF: {AMSDF_ham.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a337e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangular_window = np.ones(frame_length)\n",
    "STE_rect, STM_rect, ZCR_rect, AC_rect, AMDF_rect, AMSDF_rect = extract_features(frames, rectangular_window)\n",
    "\n",
    "print(\"Rectangular Window Features:\")\n",
    "print(f\"STE: {STE_rect.shape}, STM: {STM_rect.shape}, ZCR: {ZCR_rect.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_time = np.arange(num_frames) * frame_shift / sr\n",
    "\n",
    "fig, axes = plt.subplots(6, 2, figsize=(16, 18))\n",
    "\n",
    "axes[0, 0].plot(frame_time, STE_ham, linewidth=0.8, color='blue')\n",
    "axes[0, 0].set_title('STE (Hamming)')\n",
    "axes[0, 0].set_ylabel('Energy')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(frame_time, STE_rect, linewidth=0.8, color='red')\n",
    "axes[0, 1].set_title('STE (Rectangular)')\n",
    "axes[0, 1].set_ylabel('Energy')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[1, 0].plot(frame_time, STM_ham, linewidth=0.8, color='blue')\n",
    "axes[1, 0].set_title('STM (Hamming)')\n",
    "axes[1, 0].set_ylabel('Magnitude')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(frame_time, STM_rect, linewidth=0.8, color='red')\n",
    "axes[1, 1].set_title('STM (Rectangular)')\n",
    "axes[1, 1].set_ylabel('Magnitude')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "axes[2, 0].plot(frame_time, ZCR_ham, linewidth=0.8, color='blue')\n",
    "axes[2, 0].set_title('ZCR (Hamming)')\n",
    "axes[2, 0].set_ylabel('ZCR')\n",
    "axes[2, 0].grid(True)\n",
    "\n",
    "axes[2, 1].plot(frame_time, ZCR_rect, linewidth=0.8, color='red')\n",
    "axes[2, 1].set_title('ZCR (Rectangular)')\n",
    "axes[2, 1].set_ylabel('ZCR')\n",
    "axes[2, 1].grid(True)\n",
    "\n",
    "axes[3, 0].imshow(AC_ham[:, :200].T, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[3, 0].set_title('Autocorrelation (Hamming)')\n",
    "axes[3, 0].set_ylabel('Lag')\n",
    "\n",
    "axes[3, 1].imshow(AC_rect[:, :200].T, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[3, 1].set_title('Autocorrelation (Rectangular)')\n",
    "axes[3, 1].set_ylabel('Lag')\n",
    "\n",
    "axes[4, 0].imshow(AMDF_ham[:, :200].T, aspect='auto', origin='lower', cmap='plasma')\n",
    "axes[4, 0].set_title('AMDF (Hamming)')\n",
    "axes[4, 0].set_ylabel('Lag')\n",
    "\n",
    "axes[4, 1].imshow(AMDF_rect[:, :200].T, aspect='auto', origin='lower', cmap='plasma')\n",
    "axes[4, 1].set_title('AMDF (Rectangular)')\n",
    "axes[4, 1].set_ylabel('Lag')\n",
    "\n",
    "axes[5, 0].imshow(AMSDF_ham[:, :200].T, aspect='auto', origin='lower', cmap='inferno')\n",
    "axes[5, 0].set_title('AMSDF (Hamming)')\n",
    "axes[5, 0].set_xlabel('Time (s)')\n",
    "axes[5, 0].set_ylabel('Lag')\n",
    "\n",
    "axes[5, 1].imshow(AMSDF_rect[:, :200].T, aspect='auto', origin='lower', cmap='inferno')\n",
    "axes[5, 1].set_title('AMSDF (Rectangular)')\n",
    "axes[5, 1].set_xlabel('Time (s)')\n",
    "axes[5, 1].set_ylabel('Lag')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac1fb1",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "Voiced speech shows high energy, low ZCR, strong periodicity peaks. Unvoiced speech shows low energy, high ZCR, weak periodicity. Hamming window provides smoother features and better discrimination than rectangular window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a2804",
   "metadata": {},
   "source": [
    "Task 2: Periodicity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_threshold = np.percentile(STE_ham, 30)\n",
    "zcr_threshold = np.percentile(ZCR_ham, 70)\n",
    "\n",
    "periodic_frames = []\n",
    "\n",
    "for i in range(num_frames):\n",
    "    ac_frame = AC_ham[i, 20:200]\n",
    "    max_peak = np.max(ac_frame)\n",
    "    \n",
    "    is_periodic = (STE_ham[i] > energy_threshold and \n",
    "                   ZCR_ham[i] < zcr_threshold and \n",
    "                   max_peak > 0.3)\n",
    "    \n",
    "    periodic_frames.append(1 if is_periodic else 0)\n",
    "\n",
    "periodic_frames = np.array(periodic_frames)\n",
    "\n",
    "print(f\"Total Frames: {num_frames}\")\n",
    "print(f\"Periodic Frames: {np.sum(periodic_frames)}\")\n",
    "print(f\"Aperiodic Frames: {num_frames - np.sum(periodic_frames)}\")\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(frame_time, periodic_frames)\n",
    "plt.title(\"Periodic (1) vs Aperiodic (0) Frames\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Periodicity\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a651332",
   "metadata": {},
   "source": [
    "**Periodicity Reflection:**\n",
    "\n",
    "Autocorrelation exhibits peaks at pitch period for voiced speech. AMDF and AMSDF show minima at pitch period. Aperiodic frames lack these patterns. These complementary measures reliably distinguish voiced from unvoiced segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789e782",
   "metadata": {},
   "source": [
    "Task 3: Pitch Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pitch(autocorr, amdf, amsdf, sr, min_lag=20, max_lag=200):\n",
    "    ac_region = autocorr[min_lag:max_lag]\n",
    "    peak_lag = np.argmax(ac_region) + min_lag\n",
    "    pitch_ac = sr / peak_lag if np.max(ac_region) > 0.3 else 0\n",
    "    \n",
    "    amdf_region = amdf[min_lag:max_lag]\n",
    "    min_lag_amdf = np.argmin(amdf_region) + min_lag\n",
    "    pitch_amdf = sr / min_lag_amdf if np.min(amdf_region) < 0.7 * np.mean(amdf_region) else 0\n",
    "    \n",
    "    amsdf_region = amsdf[min_lag:max_lag]\n",
    "    min_lag_amsdf = np.argmin(amsdf_region) + min_lag\n",
    "    pitch_amsdf = sr / min_lag_amsdf if np.min(amsdf_region) < 0.7 * np.mean(amsdf_region) else 0\n",
    "    \n",
    "    return pitch_ac, pitch_amdf, pitch_amsdf\n",
    "\n",
    "pitch_ac = []\n",
    "pitch_amdf = []\n",
    "pitch_amsdf = []\n",
    "\n",
    "for i in range(num_frames):\n",
    "    if periodic_frames[i]:\n",
    "        p_ac, p_amdf, p_amsdf = estimate_pitch(AC_ham[i], AMDF_ham[i], AMSDF_ham[i], sr)\n",
    "        pitch_ac.append(p_ac)\n",
    "        pitch_amdf.append(p_amdf)\n",
    "        pitch_amsdf.append(p_amsdf)\n",
    "    else:\n",
    "        pitch_ac.append(0)\n",
    "        pitch_amdf.append(0)\n",
    "        pitch_amsdf.append(0)\n",
    "\n",
    "pitch_ac = np.array(pitch_ac)\n",
    "pitch_amdf = np.array(pitch_amdf)\n",
    "pitch_amsdf = np.array(pitch_amsdf)\n",
    "\n",
    "valid_ac = pitch_ac[pitch_ac > 0]\n",
    "valid_amdf = pitch_amdf[pitch_amdf > 0]\n",
    "valid_amsdf = pitch_amsdf[pitch_amsdf > 0]\n",
    "\n",
    "print(\"Pitch Statistics:\")\n",
    "print(f\"Autocorrelation - Mean: {np.mean(valid_ac):.2f} Hz, Range: {np.min(valid_ac):.2f}-{np.max(valid_ac):.2f} Hz\")\n",
    "print(f\"AMDF - Mean: {np.mean(valid_amdf):.2f} Hz, Range: {np.min(valid_amdf):.2f}-{np.max(valid_amdf):.2f} Hz\")\n",
    "print(f\"AMSDF - Mean: {np.mean(valid_amsdf):.2f} Hz, Range: {np.min(valid_amsdf):.2f}-{np.max(valid_amsdf):.2f} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_ac_plot = np.copy(pitch_ac)\n",
    "pitch_amdf_plot = np.copy(pitch_amdf)\n",
    "pitch_amsdf_plot = np.copy(pitch_amsdf)\n",
    "\n",
    "pitch_ac_plot[pitch_ac_plot == 0] = np.nan\n",
    "pitch_amdf_plot[pitch_amdf_plot == 0] = np.nan\n",
    "pitch_amsdf_plot[pitch_amsdf_plot == 0] = np.nan\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 5))\n",
    "\n",
    "ax.plot(frame_time, pitch_ac_plot, linewidth=1.5, marker='o', markersize=2, label='Autocorrelation', alpha=0.8)\n",
    "ax.plot(frame_time, pitch_amdf_plot, linewidth=1.5, marker='s', markersize=2, label='AMDF', alpha=0.8)\n",
    "ax.plot(frame_time, pitch_amsdf_plot, linewidth=1.5, marker='^', markersize=2, label='AMSDF', alpha=0.8)\n",
    "\n",
    "ax.set_title('Pitch Contour Comparison')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Pitch Frequency (Hz)')\n",
    "ax.set_ylim([50, 500])\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3332e",
   "metadata": {},
   "source": [
    "**Pitch Results:**\n",
    "\n",
    "Estimated pitch frequencies fall within human speech range (80-300 Hz). All three methods show consistent trends. Autocorrelation provides smoothest contours. Methods demonstrate strong agreement confirming reliable detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c126939",
   "metadata": {},
   "source": [
    "Task 4: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a3655",
   "metadata": {},
   "source": [
    "**a) Voiced vs Unvoiced Comparison:**\n",
    "\n",
    "Voiced speech exhibits high energy, low ZCR, and strong periodicity due to vocal cord vibration. Unvoiced speech shows low energy, high ZCR, and weak periodicity from turbulent airflow. Combining these features provides robust voiced/unvoiced classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e347b",
   "metadata": {},
   "source": [
    "**b) Periodicity-Based Pitch Detection:**\n",
    "\n",
    "Autocorrelation identifies pitch through repetition peaks. AMDF and AMSDF highlight pitch period via minima. These time-domain methods directly exploit vocal cord periodicity. Peak/minimum locations correspond to fundamental frequency enabling accurate pitch estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96ae4c",
   "metadata": {},
   "source": [
    "**c) Feature Reliability:**\n",
    "\n",
    "Energy and ZCR effectively identify voiced regions but cannot estimate pitch directly. Autocorrelation is most widely used and reliable for clean speech. AMSDF provides enhanced precision with sharper minima. AMDF offers computational efficiency with comparable accuracy. All three periodicity methods demonstrate strong inter-method agreement."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
